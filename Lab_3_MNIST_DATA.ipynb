{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ekWEvs4_f5C"
      },
      "source": [
        "Angel Martinez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv5HnW0D3o4a"
      },
      "outputs": [],
      "source": [
        "#we imported data from the keras library\n",
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels)=mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO75LBQx6dVJ"
      },
      "outputs": [],
      "source": [
        "#after importing the dataset, we exported layers and modesl from keras as well. the reason we have 512 while when we multiply by 28*28 is 784 but the relu reduced it\n",
        "from keras import models\n",
        "from keras import layers\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuZX-uQ87z4y"
      },
      "outputs": [],
      "source": [
        "# we are reshaping the data to fit the expecation of the network\n",
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xwOmyVL73i4"
      },
      "outputs": [],
      "source": [
        "#categorically encoding the labels\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7N_GI5H8D52"
      },
      "outputs": [],
      "source": [
        "#preparing the labels, using the tool from keras\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6jssJz19rrW",
        "outputId": "9f531497-66f5-4863-9ef4-6be24900cb45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 9ms/step - loss: 0.2658 - accuracy: 0.9231\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.1078 - accuracy: 0.9682\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0721 - accuracy: 0.9785\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0514 - accuracy: 0.9845\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0389 - accuracy: 0.9882\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c6a8f5ec820>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#training the network, which acounts for errors and other mishaps\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_BSAnrY-COV",
        "outputId": "03e16f8a-32f7-408a-a997-c1b3a5a28f43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0643 - accuracy: 0.9790\n",
            "test_acc: 0.9789999723434448\n"
          ]
        }
      ],
      "source": [
        "#testing the dataset and it reached an accuracy of .9789 or 98%\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "print('test_acc:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeHWvxvI-UKE"
      },
      "source": [
        "ChatGPT coding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEmhYBSb-fYE",
        "outputId": "91557624-3b33-4b16-f848-6fb2daeffd3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "750/750 [==============================] - 64s 83ms/step - loss: 0.2179 - accuracy: 0.9356 - val_loss: 0.0696 - val_accuracy: 0.9819\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 44s 58ms/step - loss: 0.0584 - accuracy: 0.9816 - val_loss: 0.0490 - val_accuracy: 0.9867\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 0.0402 - accuracy: 0.9867 - val_loss: 0.0385 - val_accuracy: 0.9889\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 0.0316 - accuracy: 0.9898 - val_loss: 0.0493 - val_accuracy: 0.9855\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 0.0257 - accuracy: 0.9918 - val_loss: 0.0376 - val_accuracy: 0.9893\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0277 - accuracy: 0.9918\n",
            "Test accuracy: 99.18%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# Build the CNN model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model, we used the common optimizer 'adam' for dl models.\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model, we used the batch size of 64, valid. split of .2, and epoch of 5(which is interchagneable to fit how many times you want the program to run)\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_acc * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTd74Zry_0I5"
      },
      "source": [
        "This the data we got from chatgpt, once we run it, it did import the tensorflow as well.Which we thought would give us error but it did not. It is pretty similar to the data we already have and the accuracy we got was 98%, which is pretty good. W ebelieve it uses different model but give us same result."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
